import os
import time
import torch
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from torch import nn
from dataset import CatDogDataset
# from model import build_vgg16_model, unfreeze_last_conv_block
from model import build_vgg16_model, unfreeze_all_layers


class CatDogTrainer:
    def __init__(self, train_dir, test_dir, transform, epochs=10, fine_tune_epochs=10, fine_tune_lr=0.0001):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨ã€‚
        å‚æ•°:
            train_dir (str): è®­ç»ƒæ•°æ®ç›®å½•ã€‚
            test_dir (str): æµ‹è¯•æ•°æ®ç›®å½•ã€‚
            transform (callable): å›¾åƒé¢„å¤„ç†ã€‚
            epochs (int): ç¬¬ä¸€é˜¶æ®µï¼ˆè¿ç§»å­¦ä¹ ï¼‰çš„è®­ç»ƒè½®æ•°ã€‚
            fine_tune_epochs (int): ç¬¬äºŒé˜¶æ®µï¼ˆå¾®è°ƒï¼‰çš„è®­ç»ƒè½®æ•°ã€‚
            fine_tune_lr (float): å¾®è°ƒé˜¶æ®µçš„å­¦ä¹ ç‡ã€‚
        """
        self.train_dir = train_dir
        self.test_dir = test_dir
        self.transform = transform
        self.epochs = epochs
        self.fine_tune_epochs = fine_tune_epochs
        self.fine_tune_lr = fine_tune_lr

        self.batch_size = 32
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # build_vgg16_model() ä¸å†éœ€è¦å‚æ•°
        self.model = build_vgg16_model().to(self.device)

        self.checkpoint_path = "checkpoints/best_catdog_vgg16.pth"
        self.loss_fn = nn.CrossEntropyLoss()

        # å†å²è®°å½•
        self.history = {
            'stage1': {'train_loss': [], 'test_loss': [], 'test_acc': []},
            'stage2': {'train_loss': [], 'test_loss': [], 'test_acc': []}
        }
        self.best_acc = 0.0  # ç”¨äºè¿½è¸ªæœ€ä½³å‡†ç¡®ç‡

    def load_data(self):
        """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚"""
        train_dataset = CatDogDataset(self.train_dir, self.transform)
        test_dataset = CatDogDataset(self.test_dir, self.transform)
        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
        print(f"æ•°æ®åŠ è½½å®Œæ¯•ï¼šè®­ç»ƒé›† {len(train_dataset)} å¼ ï¼Œæµ‹è¯•é›† {len(test_dataset)} å¼ ã€‚")

    def save_model(self, current_acc):
        """ä»…å½“æ¨¡å‹æ€§èƒ½æå‡æ—¶ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹ã€‚"""
        if current_acc > self.best_acc:
            self.best_acc = current_acc
            os.makedirs(os.path.dirname(self.checkpoint_path), exist_ok=True)
            torch.save(self.model.state_dict(), self.checkpoint_path)
            print(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼å‡†ç¡®ç‡: {current_acc * 100:.2f}%")

    def evaluate(self, loader):
        """åœ¨ç»™å®šçš„æ•°æ®åŠ è½½å™¨ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œè¿”å›å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡ã€‚"""
        self.model.eval()
        correct = 0
        total_loss = 0
        dataset_size = len(loader.dataset)

        with torch.no_grad():
            for x, y in loader:
                x, y = x.to(self.device), y.to(self.device)
                pred = self.model(x)
                loss = self.loss_fn(pred, y)
                total_loss += loss.item() * x.size(0)
                pred_labels = torch.argmax(pred, dim=1)
                correct += (pred_labels == y).sum().item()

        avg_loss = total_loss / dataset_size
        acc = correct / dataset_size
        return avg_loss, acc

    def _train_one_epoch(self, epoch, total_epochs, optimizer):
        """æ‰§è¡Œä¸€ä¸ª epoch çš„è®­ç»ƒé€»è¾‘ã€‚"""
        self.model.train()
        epoch_train_loss_sum = 0.0
        for batch_idx, (x, y) in enumerate(self.train_loader):
            x, y = x.to(self.device), y.to(self.device)
            pred = self.model(x)
            loss = self.loss_fn(pred, y)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_train_loss_sum += loss.item() * x.size(0)
            if batch_idx % 50 == 0:
                print(
                    f"Epoch {epoch}/{total_epochs} | Batch {batch_idx}/{len(self.train_loader)} | Batch Loss: {loss.item():.4f}")

        return epoch_train_loss_sum / len(self.train_loader.dataset)

    def train(self):
        """æ‰§è¡Œå®Œæ•´çš„ä¸¤é˜¶æ®µè®­ç»ƒï¼šè¿ç§»å­¦ä¹  + å¾®è°ƒã€‚"""
        print(f"è®¾å¤‡: {self.device}")
        start_time = time.time()

        # --- é˜¶æ®µä¸€ï¼šè¿ç§»å­¦ä¹ ï¼ˆåªè®­ç»ƒåˆ†ç±»å¤´ï¼‰ ---
        print("\n--- å¼€å§‹ç¬¬ä¸€é˜¶æ®µï¼šè¿ç§»å­¦ä¹  (Feature Extraction) ---")
        params_to_update_stage1 = [p for p in self.model.parameters() if p.requires_grad]
        # optimizer_stage1 = torch.optim.SGD(params_to_update_stage1, lr=0.01, momentum=0.9)
        # åœ¨ CatDogTrainer ç±»çš„ train æ–¹æ³•ä¸­
        optimizer_stage1 = torch.optim.SGD(params_to_update_stage1, lr=0.001, momentum=0.9)

        for epoch in range(1, self.epochs + 1):
            avg_train_loss = self._train_one_epoch(epoch, self.epochs, optimizer_stage1)
            avg_test_loss, avg_test_acc = self.evaluate(self.test_loader)

            print(f"--- [é˜¶æ®µä¸€] Epoch {epoch}/{self.epochs} æ€»ç»“ ---")
            print(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | æµ‹è¯•æŸå¤±: {avg_test_loss:.4f} | æµ‹è¯•å‡†ç¡®ç‡: {avg_test_acc * 100:.2f}%")

            self.history['stage1']['train_loss'].append(avg_train_loss)
            self.history['stage1']['test_loss'].append(avg_test_loss)
            self.history['stage1']['test_acc'].append(avg_test_acc)
            self.save_model(avg_test_acc)

        # --- é˜¶æ®µäºŒï¼šå¾®è°ƒï¼ˆä»…è§£å†»æœ€åçš„å·ç§¯å—å’Œåˆ†ç±»å™¨ï¼‰ ---
        print(f"\n--- å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒ (Fine-tuning last block) ---")
        # unfreeze_last_conv_block(self.model)
        unfreeze_all_layers(self.model)
        params_to_update_stage2 = [p for p in self.model.parameters() if p.requires_grad]
        print(f"\nå¾®è°ƒé˜¶æ®µå°†æ›´æ–° {len(params_to_update_stage2)} ä¸ªå‚æ•°å¼ é‡ã€‚")
        optimizer_stage2 = torch.optim.SGD(params_to_update_stage2, lr=self.fine_tune_lr, momentum=0.9)

        total_epochs_combined = self.epochs + self.fine_tune_epochs
        for epoch in range(self.epochs + 1, total_epochs_combined + 1):
            current_fine_tune_epoch = epoch - self.epochs
            avg_train_loss = self._train_one_epoch(f"{current_fine_tune_epoch}/{self.fine_tune_epochs}", "å¾®è°ƒ",
                                                 optimizer_stage2)
            avg_test_loss, avg_test_acc = self.evaluate(self.test_loader)

            print(f"--- [é˜¶æ®µäºŒ] Epoch {current_fine_tune_epoch}/{self.fine_tune_epochs} æ€»ç»“ ---")
            print(f"è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | æµ‹è¯•æŸå¤±: {avg_test_loss:.4f} | æµ‹è¯•å‡†ç¡®ç‡: {avg_test_acc * 100:.2f}%")

            self.history['stage2']['train_loss'].append(avg_train_loss)
            self.history['stage2']['test_loss'].append(avg_test_loss)
            self.history['stage2']['test_acc'].append(avg_test_acc)
            self.save_model(avg_test_acc)

        training_duration = time.time() - start_time
        print(f"\nè®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶: {training_duration:.1f} ç§’ ({training_duration / 60:.2f} åˆ†é’Ÿ)")
        print(f"ğŸ† æœ€é«˜æµ‹è¯•å‡†ç¡®ç‡: {self.best_acc * 100:.2f}%ï¼Œæ¨¡å‹å·²ä¿å­˜åœ¨ {self.checkpoint_path}")

    def plot_curves(self):
        """ç»˜åˆ¶æ›²çº¿å›¾ï¼Œä½¿ç”¨å…¨å±€è®¾ç½®æ˜¾ç¤ºä¸­æ–‡ã€‚"""
        # ç”¨ä¸¤è¡Œä»£ç è®¾ç½®å…¨å±€ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']  # æŒ‡å®šä¸­æ–‡å­—ä½“
        plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·

        plt.figure(figsize=(18, 6))

        epochs_stage1 = range(1, self.epochs + 1)
        epochs_stage2 = range(self.epochs + 1, self.epochs + self.fine_tune_epochs + 1)

        # 1. æŸå¤±æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.title('è®­ç»ƒä¸æµ‹è¯•æŸå¤±æ›²çº¿')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')

        plt.plot(epochs_stage1, self.history['stage1']['train_loss'], 'bo-', label='é˜¶æ®µ1 - è®­ç»ƒæŸå¤±')
        plt.plot(epochs_stage1, self.history['stage1']['test_loss'], 'ro-', label='é˜¶æ®µ1 - æµ‹è¯•æŸå¤±')
        if self.fine_tune_epochs > 0:
            plt.plot(epochs_stage2, self.history['stage2']['train_loss'], 'g*--', label='é˜¶æ®µ2 - è®­ç»ƒæŸå¤±')
            plt.plot(epochs_stage2, self.history['stage2']['test_loss'], 'm*--', label='é˜¶æ®µ2 - æµ‹è¯•æŸå¤±')
            plt.axvline(x=self.epochs + 0.5, color='gray', linestyle='--', label='é˜¶æ®µåˆ†å‰²çº¿')

        plt.legend()
        plt.grid(True)

        # 2. å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(1, 2, 2)
        plt.title('æµ‹è¯•å‡†ç¡®ç‡æ›²çº¿')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')

        plt.plot(epochs_stage1, self.history['stage1']['test_acc'], 'ro-', label='é˜¶æ®µ1 - æµ‹è¯•å‡†ç¡®ç‡')
        if self.fine_tune_epochs > 0:
            plt.plot(epochs_stage2, self.history['stage2']['test_acc'], 'm*--', label='é˜¶æ®µ2 - æµ‹è¯•å‡†ç¡®ç‡')
            plt.axvline(x=self.epochs + 0.5, color='gray', linestyle='--', label='é˜¶æ®µåˆ†å‰²çº¿')

        plt.legend()
        plt.grid(True)

        plot_filename = "training_curves_fine_tuned.png"
        plt.savefig(plot_filename)
        print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜è‡³ {plot_filename}")
        plt.close('all')

    def run(self):
        """æ‰§è¡Œè®­ç»ƒå™¨çš„ä¸»æµç¨‹ã€‚"""
        self.load_data()
        self.train()
        self.plot_curves()