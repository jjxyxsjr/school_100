import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import os
import time
import copy
import torch.nn.functional as F
from PIL import Image  # ç¡®ä¿å¯¼å…¥Image


# ==============================================================================
#  å‡½æ•°å®šä¹‰éƒ¨åˆ†
# ==============================================================================

# train_model å’Œ plot_curves å‡½æ•°ä¿æŒä¸å˜
def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, num_epochs=10, device='cpu'):
    since = time.time()
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print(f'Epoch {epoch + 1}/{num_epochs}')
        print('-' * 10)
        for phase in ['train', 'validation']:
            if phase == 'train':
                model.train()
            else:
                model.eval()
            running_loss = 0.0
            running_corrects = 0
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)
                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]
            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            if phase == 'train':
                history['train_loss'].append(epoch_loss)
                history['train_acc'].append(epoch_acc.item())
            else:
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item())
            if phase == 'validation' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
        print()
    time_elapsed = time.time() - since
    print(f'è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_acc:4f}')
    model.load_state_dict(best_model_wts)
    return model, history


def plot_curves(history):
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_acc'], label='Train Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend()
    plt.tight_layout()
    plt.show()


def test_model_with_tta(model, test_loader, tta_transforms, device='cpu'):
    print("\nå¼€å§‹ä½¿ç”¨ TTA åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    model.eval()
    total_corrects = 0
    total_images = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            # inputs æ˜¯ä¸€ä¸ªåŒ…å« PIL Image çš„ list, labels æ˜¯ä¸€ä¸ªåŒ…å« int çš„ tensor
            image = inputs[0]  # å–å‡º list ä¸­çš„ PIL Image
            label = labels[0].to(device)

            augmented_images = torch.stack([tf(image) for tf in tta_transforms]).to(device)
            outputs = model(augmented_images)
            mean_outputs = torch.mean(outputs, dim=0)
            _, pred = torch.max(mean_outputs, 0)

            if pred == label:
                total_corrects += 1
            total_images += 1

    tta_acc = total_corrects / total_images
    print(f'ä½¿ç”¨ TTA çš„æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡: {tta_acc:.4f} ğŸ¯ğŸš€')


# !!! æ–°å¢ï¼šè‡ªå®šä¹‰çš„ collate_fn å‡½æ•° !!!
def tta_collate_fn(batch):
    """
    è‡ªå®šä¹‰çš„collate_fnï¼Œç”¨äºå¤„ç†åŒ…å«PIL Imageçš„batchã€‚
    batch: ä¸€ä¸ªlistï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ (PIL_image, label) çš„å…ƒç»„ã€‚
    """
    images = [item[0] for item in batch]  # å°†PIL Imageæ”¶é›†åˆ°listä¸­
    labels = [item[1] for item in batch]  # å°†labelæ”¶é›†åˆ°listä¸­
    labels = torch.tensor(labels)  # å°†label listè½¬æ¢ä¸ºtensor

    return images, labels


# ==============================================================================
#  ä¸»ç¨‹åºæ‰§è¡Œå—
# ==============================================================================
if __name__ == '__main__':
    # 1. è®¾ç½®è¶…å‚æ•°å’Œè®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨çš„è®¾å¤‡æ˜¯: {device} âœ¨")
    data_dir = 'flower_photos'
    batch_size = 32
    num_epochs = 10
    learning_rate = 0.001

    # 2. æ•°æ®é¢„å¤„ç†ä¸åŠ è½½
    train_val_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'validation': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    tta_transforms = [
        transforms.Compose([
            transforms.Resize((224, 224)),  # ç›´æ¥resizeï¼Œä¸è£å‰ª
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p=1.0),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    ]

    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), train_val_transforms[x])
                      for x in ['train', 'validation']}

    test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=None)
    image_datasets['test'] = test_dataset

    dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4)
                   for x in ['train', 'validation']}

    # !!! å…³é”®ä¿®å¤ï¼šåœ¨DataLoaderä¸­ä½¿ç”¨è‡ªå®šä¹‰çš„collate_fn !!!
    test_loader_for_tta = DataLoader(image_datasets['test'],
                                     batch_size=1,
                                     shuffle=False,
                                     num_workers=4,
                                     collate_fn=tta_collate_fn)

    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation', 'test']}
    class_names = image_datasets['train'].classes
    num_classes = len(class_names)
    print("æ•°æ®åŠ è½½å®Œæˆï¼")
    print(
        f"è®­ç»ƒé›†å¤§å°: {dataset_sizes['train']}, éªŒè¯é›†å¤§å°: {dataset_sizes['validation']}, æµ‹è¯•é›†å¤§å°: {dataset_sizes['test']}")
    print(f"ç±»åˆ«æ•°é‡: {num_classes}, ç±»åˆ«åç§°: {class_names}")

    # 3. æ„å»ºæ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
    for param in model.parameters():
        param.requires_grad = False
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, num_classes)
    model = model.to(device)
    print("æ¨¡å‹æ„å»ºå®Œæˆï¼Œå·²æ›¿æ¢åˆ†ç±»å¤´å¹¶å†»ç»“ä¸»å¹²ç½‘ç»œã€‚")
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)

    # 4. è®­ç»ƒå’Œç»˜å›¾
    trained_model, history = train_model(model, criterion, optimizer, dataloaders, dataset_sizes, num_epochs, device)
    print("\næ­£åœ¨ç»˜åˆ¶è®­ç»ƒæ›²çº¿å›¾...")
    plot_curves(history)

    # 5. è°ƒç”¨TTAæµ‹è¯•å‡½æ•°
    test_model_with_tta(trained_model, test_loader_for_tta, tta_transforms, device)